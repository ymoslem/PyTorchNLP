{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLIsU71hjqFz4P11Kt9s88",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ymoslem/PyTorchNLP/blob/main/Ex6-NMT-Transformer-custom-dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NMT with PyTorch nn.Transformer\n",
        "\n",
        "* Paper: [Attention is all you need](https://arxiv.org/abs/1706.03762)\n",
        "\n",
        "* PyTorch Transformer Classs: https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "\n",
        "* Reference video: https://www.youtube.com/watch?v=M6adRGJe5cQ"
      ],
      "metadata": {
        "id": "jewzKjJ27sHB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-07-21T20:21:01.578512Z",
          "start_time": "2022-07-21T20:21:00.915675Z"
        },
        "id": "b9cd7ab3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from torchtext.data.functional import generate_sp_model, load_sp_model, sentencepiece_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from collections import Counter\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "from itertools import islice"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget -qq https://nmt-datasets.s3.us-west-2.amazonaws.com/ar-en/Tatoeba.ar-en.ar-filtered.ar.semantic.bz2\n",
        "# !wget -qq https://nmt-datasets.s3.us-west-2.amazonaws.com/ar-en/Tatoeba.ar-en.en-filtered.en.semantic.bz2\n",
        "\n",
        "# !bzip2 -d Tatoeba.ar-en.ar-filtered.ar.semantic.bz2\n",
        "# !bzip2 -d Tatoeba.ar-en.en-filtered.en.semantic.bz2\n",
        "\n",
        "# !mkdir data\n",
        "# !mv data/Tatoeba.ar-en.ar-filtered.ar.semantic data/Tatoeba.ar-en.ar\n",
        "# !mv data/Tatoeba.ar-en.en-filtered.en.semantic data/Tatoeba.ar-en.en"
      ],
      "metadata": {
        "id": "Vm6JLENGh-hJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-07-21T20:21:01.582776Z",
          "start_time": "2022-07-21T20:21:01.580091Z"
        },
        "id": "013cf7db"
      },
      "outputs": [],
      "source": [
        "source_file = \"data/Tatoeba.ar-en.en\"\n",
        "target_file = \"data/Tatoeba.ar-en.ar\"\n",
        "\n",
        "src_code = \"<en>\"\n",
        "tgt_code = \"<ar>\"\n",
        "eos_code = \"</s>\"\n",
        "\n",
        "source_file_tok = source_file+\".tok\"\n",
        "target_file_tok = target_file+\".tok\"\n",
        "\n",
        "lower = True\n",
        "dev_size = 1000\n",
        "test_size = 1000\n",
        "\n",
        "max_vocab_size = 32000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-07-21T20:21:01.650780Z",
          "start_time": "2022-07-21T20:21:01.584153Z"
        },
        "id": "af36b56e",
        "outputId": "47ff068d-9cac-4531-808e-b10eff7ee5ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Vocab Size: 2402\n",
            "Target Vocab Size: 4640\n"
          ]
        }
      ],
      "source": [
        "# Count words in the file\n",
        "# [To-Do] split up to 10 million lines for SentencePiece\n",
        "\n",
        "def get_data_freq(data_file, min_freq=2, max_vocab_size=32000):\n",
        "    with open(data_file) as data:\n",
        "        text = data.read()\n",
        "        frequent_word_count = list(Counter(text.split()).values()).count(min_freq)\n",
        "        vocab_size = frequent_word_count if frequent_word_count < max_vocab_size else max_vocab_size\n",
        "    return vocab_size\n",
        "\n",
        "\n",
        "source_vocab_size = get_data_freq(data_file=source_file, min_freq=2, max_vocab_size=max_vocab_size)\n",
        "print(\"Source Vocab Size:\", source_vocab_size)\n",
        "\n",
        "target_vocab_size = get_data_freq(data_file=target_file, min_freq=2, max_vocab_size=max_vocab_size)\n",
        "print(\"Target Vocab Size:\", target_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-07-21T20:21:03.526625Z",
          "start_time": "2022-07-21T20:21:01.652283Z"
        },
        "id": "feaf596f",
        "outputId": "92b5458c-04dc-4d4b-f83d-a920518f868d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done! Training SentencePiece source model completed.\n",
            "Done! Training SentencePiece target model completed.\n"
          ]
        }
      ],
      "source": [
        "# Train a SentencePiece model for the source\n",
        "generate_sp_model(source_file,\n",
        "                  vocab_size=source_vocab_size,\n",
        "                  model_type=\"unigram\",\n",
        "                  model_prefix=\"source.spm\")\n",
        "print(\"Done! Training SentencePiece source model completed.\")\n",
        "\n",
        "# Train a SentencePiece model for the target\n",
        "generate_sp_model(target_file,\n",
        "                  vocab_size=target_vocab_size,\n",
        "                  model_type=\"unigram\",\n",
        "                  model_prefix=\"target.spm\")\n",
        "print(\"Done! Training SentencePiece target model completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-07-21T20:21:03.539887Z",
          "start_time": "2022-07-21T20:21:03.528255Z"
        },
        "id": "1ee9301d"
      },
      "outputs": [],
      "source": [
        "# Load the SentencePiece models\n",
        "source_sp_model = load_sp_model(\"source.spm.model\")\n",
        "target_sp_model = load_sp_model(\"target.spm.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-07-21T20:21:04.314380Z",
          "start_time": "2022-07-21T20:21:03.550557Z"
        },
        "id": "0f5661ec",
        "outputId": "2dc4ce16-341c-4968-8bed-601cdaed5528",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset size: 24714 | Dev dataset size: 1000 | Test dataset size: 1000\n"
          ]
        }
      ],
      "source": [
        "# Tokenize data and split it into train, dev, test\n",
        "def prepare(source_file, target_file, dev_size, test_size, source_sp_model, target_sp_model, lower=False):\n",
        "    with open(source_file) as source, open(target_file) as target:\n",
        "        if lower==True:\n",
        "            source_lines = [line.strip().lower() for line in source.readlines()]\n",
        "            target_lines = [line.strip().lower() for line in target.readlines()]\n",
        "        else:\n",
        "            source_lines = [line.strip() for line in source.readlines()]\n",
        "            target_lines = [line.strip() for line in target.readlines()]\n",
        "\n",
        "\n",
        "        if len(source_lines) == len(target_lines):\n",
        "            data_size = len(source_lines)\n",
        "            # print(data_size)\n",
        "        else:\n",
        "            raise ValueError(\"Length of source and target lines must be the same!\")\n",
        "\n",
        "        # Tokenize source sentences\n",
        "        sp_tokens_generator = sentencepiece_tokenizer(source_sp_model)\n",
        "        source_lines_tok = sp_tokens_generator(source_lines)\n",
        "\n",
        "        # Tokenize target sentences\n",
        "        sp_tokens_generator = sentencepiece_tokenizer(target_sp_model)\n",
        "        target_lines_tok = sp_tokens_generator(target_lines)\n",
        "\n",
        "        # Split data into train, dev, and test\n",
        "        data_lines = zip(source_lines_tok, target_lines_tok)\n",
        "\n",
        "        train_size = data_size - (dev_size + test_size)\n",
        "        # print(train_size)\n",
        "\n",
        "        dev_dataset = list(islice(data_lines, dev_size))\n",
        "        # print(next(iter(dev_dataset)))\n",
        "        test_dataset = list(islice(data_lines, test_size))\n",
        "        # print(next(iter(test_dataset)))\n",
        "        train_dataset = list(islice(data_lines, train_size))\n",
        "        # print(next(iter(train_dataset)))\n",
        "\n",
        "        return train_dataset, dev_dataset, test_dataset\n",
        "\n",
        "train_dataset, dev_dataset, test_dataset = prepare(source_file, target_file, dev_size, test_size,\n",
        "                                                   source_sp_model, target_sp_model,\n",
        "                                                   lower=lower)\n",
        "\n",
        "print(\"Training dataset size: %s\" %len(train_dataset),\n",
        "      \"Dev dataset size: %s\" %len(dev_dataset),\n",
        "      \"Test dataset size: %s\" %len(test_dataset),\n",
        "      sep=\" | \"\n",
        "     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-07-21T20:21:04.319315Z",
          "start_time": "2022-07-21T20:21:04.315771Z"
        },
        "id": "92bb80b8",
        "outputId": "a1004e3e-9c41-427e-f63c-a3fa6ed3f775",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁', 'i', '▁hate', '▁this', '▁weather', '.']\n",
            "['▁this', '▁movie', '▁is', '▁worth', '▁see', 'ing', '▁again', '.']\n",
            "['▁who', '▁teach', 'es', '▁you', '▁f', 're', 'n', 'ch', '?']\n",
            "['▁', 'i', '▁fel', 't', '▁sad', '.']\n",
            "['▁we', '▁have', '▁to', '▁help', '.']\n",
            "['▁كم', '▁أكره', '▁هذا', '▁الطقس', '!']\n",
            "['▁هذا', '▁الفلم', '▁يستحق', '▁ال', 'مشاهدة', '▁لم', 'رة', '▁ثانية', '.']\n",
            "['▁من', '▁يدر', 'ّ', 'سك', '▁الفرنسية', '؟']\n",
            "['▁شعر', 'ت', '▁بال', 'حزن', '.']\n",
            "['▁علينا', '▁أن', '▁ن', 'ساعد']\n"
          ]
        }
      ],
      "source": [
        "source, target = zip(*test_dataset)\n",
        "print(*source[:5], sep=\"\\n\")\n",
        "print(*target[:5], sep=\"\\n\")\n",
        "\n",
        "#print(*test_dataset[:5], sep=\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-07-21T20:21:04.547832Z",
          "start_time": "2022-07-21T20:21:04.320365Z"
        },
        "id": "3b747bd7",
        "outputId": "0b15410b-a0d1-4229-e80b-edbaaf833751",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First source sentence: ['▁that', '▁clinic', '▁still', '▁exist', 's', '.']\n",
            "First target sentence: ['▁تلك', '▁العيادة', '▁لا', '▁ت', 'زال', '▁موجودة', '.']\n",
            "Source vocab Size: 2075\n",
            "Target vocab Size: 4665\n"
          ]
        }
      ],
      "source": [
        "# Build Vocabulary\n",
        "\n",
        "source_train_tok, target_train_tok = zip(*train_dataset)\n",
        "print(\"First source sentence:\", next(iter(source_train_tok)))\n",
        "print(\"First target sentence:\", next(iter(target_train_tok)))\n",
        "\n",
        "# For shared vocabulary, combine the source and target, and build vocab only once\n",
        "# shared_train_tok = source_train_tok + target_train_tok\n",
        "\n",
        "# Build source vocabulary\n",
        "source_vocab = build_vocab_from_iterator(source_train_tok,\n",
        "                                     specials=[\"<unk>\", '<pad>', \"<s>\", \"</s>\", \"<en>\", \"<ar>\"],\n",
        "                                     min_freq=2,\n",
        "                                     max_tokens=max_vocab_size)\n",
        "source_vocab.set_default_index(source_vocab[\"<unk>\"])\n",
        "print(\"Source vocab Size:\", len(source_vocab))\n",
        "\n",
        "# Build target vocabulary\n",
        "target_vocab = build_vocab_from_iterator(target_train_tok,\n",
        "                                     specials=[\"<unk>\", '<pad>', \"<s>\", \"</s>\", \"<en>\", \"<ar>\"],\n",
        "                                     min_freq=2,\n",
        "                                     max_tokens=max_vocab_size)\n",
        "target_vocab.set_default_index(target_vocab[\"<unk>\"])\n",
        "print(\"Target vocab Size:\", len(target_vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-07-21T20:21:04.553285Z",
          "start_time": "2022-07-21T20:21:04.548998Z"
        },
        "id": "eef61614",
        "outputId": "597fa595-8308-413d-b875-68513030bb8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4, 95, 18, 87, 1637, 3]\n"
          ]
        }
      ],
      "source": [
        "print(source_vocab([src_code, '▁here', '▁is', '▁an', '▁example', '</s>']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-07-21T20:21:04.563116Z",
          "start_time": "2022-07-21T20:21:04.554402Z"
        },
        "id": "7737021b",
        "outputId": "455bfbef-0cb3-4166-c083-3cee53c31080",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "source_vocab[\"<pad>\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-07-21T20:21:04.571195Z",
          "start_time": "2022-07-21T20:21:04.564320Z"
        },
        "id": "fbb77122"
      },
      "outputs": [],
      "source": [
        "# Save Vocabulary if needed\n",
        "torch.save(source_vocab, 'source_vocab.pth')\n",
        "torch.save(target_vocab, 'target_vocab.pth')\n",
        "\n",
        "# How to load later\n",
        "# vocab_obj = torch.load('source_vocab.pth')\n",
        "# vocab_obj[\"<pad>\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-07-21T20:21:04.575850Z",
          "start_time": "2022-07-21T20:21:04.572830Z"
        },
        "id": "78f694e6"
      },
      "outputs": [],
      "source": [
        "# List mapping indices to tokens\n",
        "# itos = source_vocab.get_itos()\n",
        "\n",
        "# Dictionary mapping tokens to indices\n",
        "# stoi = source_vocab.get_stoi()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-07-21T20:21:04.581400Z",
          "start_time": "2022-07-21T20:21:04.577850Z"
        },
        "id": "05deacc4",
        "outputId": "62ffa8c5-5fe4-4f6a-fc38-4f51bba9c11d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Which GPU to use\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "\n",
        "# For debugging CUDA errors\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\n",
        "\n",
        "os.environ.get(\"CUDA_VISIBLE_DEVICES\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-07-21T20:21:04.634342Z",
          "start_time": "2022-07-21T20:21:04.583624Z"
        },
        "id": "319a647f"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 128  # examples\n",
        "pad_idx = target_vocab[\"<pad>\"]\n",
        "max_len = 100  # tokens in source or target\n",
        "\n",
        "\n",
        "def collate_batch(batch):\n",
        "    sources, targets = [], []\n",
        "    for source, target in batch:\n",
        "\n",
        "        if len(source) < max_len and len(target) < max_len \\\n",
        "        and len(source) > 1 and len(target) > 1:\n",
        "\n",
        "            source = [src_code] + source + [eos_code]\n",
        "            target = [tgt_code] + target + [eos_code]\n",
        "\n",
        "            source_idx = source_vocab(source)\n",
        "            target_idx = target_vocab(target)\n",
        "\n",
        "            source_tensor = torch.tensor(source_idx, dtype=torch.int64)\n",
        "            target_tensor = torch.tensor(target_idx, dtype=torch.int64)\n",
        "\n",
        "            sources.append(source_tensor)\n",
        "            targets.append(target_tensor)\n",
        "\n",
        "    sources = pad_sequence(sources, padding_value=pad_idx)\n",
        "    sources = sources.to(device)\n",
        "\n",
        "    targets = pad_sequence(targets, padding_value=pad_idx)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    return sources, targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-07-21T20:21:04.639587Z",
          "start_time": "2022-07-21T20:21:04.635807Z"
        },
        "id": "75ba7e27"
      },
      "outputs": [],
      "source": [
        "# One way without a bucket iterator\n",
        "# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
        "# valid_dataloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
        "# test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "# Test\n",
        "# for x, y in test_dataloader:\n",
        "#    print(x.shape, y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-07-21T20:21:04.645354Z",
          "start_time": "2022-07-21T20:21:04.640792Z"
        },
        "id": "c911a379"
      },
      "outputs": [],
      "source": [
        "# Alternatively, add batch_sampler to act as a bucket iterator\n",
        "# It batches examples of similar lengths together.\n",
        "# Minimizes amount of padding needed while producing freshly shuffled batches for each new epoch.\n",
        "# https://colab.research.google.com/drive/1Zg7Csa4NJ1APg5JGR0BakjOudvmqxTpt\n",
        "\n",
        "from random import shuffle\n",
        "\n",
        "def batch_sampler(dataset):\n",
        "    indices = [(i, s[1]) for i, s in enumerate(dataset)]\n",
        "    shuffle(indices)\n",
        "    pooled_indices = []\n",
        "    # create pool of indices with similar lengths\n",
        "    for i in range(0, len(indices), batch_size * 100):\n",
        "        pooled_indices.extend(sorted(indices[i:i + batch_size * 100], key=lambda x: x[1]))\n",
        "\n",
        "    pooled_indices = [x[0] for x in pooled_indices]\n",
        "\n",
        "    # yield indices for current batch\n",
        "    for i in range(0, len(pooled_indices), batch_size):\n",
        "        yield pooled_indices[i:i + batch_size]\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, collate_fn=collate_batch, batch_sampler=batch_sampler(train_dataset))\n",
        "valid_dataloader = DataLoader(dev_dataset, collate_fn=collate_batch, batch_sampler=batch_sampler(dev_dataset))\n",
        "test_dataloader = DataLoader(test_dataset, collate_fn=collate_batch, batch_sampler=batch_sampler(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-07-21T20:21:09.189246Z",
          "start_time": "2022-07-21T20:21:05.026838Z"
        },
        "id": "655ac3f5",
        "outputId": "231f40ef-58ec-4053-f6d4-d660769e91d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([55, 127]) torch.Size([46, 127])\n",
            "torch.Size([62, 126]) torch.Size([71, 126])\n",
            "torch.Size([43, 128]) torch.Size([34, 128])\n",
            "torch.Size([59, 128]) torch.Size([40, 128])\n",
            "torch.Size([35, 128]) torch.Size([44, 128])\n",
            "torch.Size([60, 128]) torch.Size([48, 128])\n",
            "torch.Size([36, 128]) torch.Size([40, 128])\n",
            "torch.Size([83, 104]) torch.Size([55, 104])\n"
          ]
        }
      ],
      "source": [
        "# Test\n",
        "for x, y in test_dataloader:\n",
        "  print(x.shape, y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04b3080e"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-07-21T20:21:09.197479Z",
          "start_time": "2022-07-21T20:21:09.190525Z"
        },
        "id": "7d4b5537"
      },
      "outputs": [],
      "source": [
        "# Info: https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_size,\n",
        "        src_vocab_size,\n",
        "        tgt_vocab_size,\n",
        "        src_pad_idx,\n",
        "        num_heads,\n",
        "        num_encoder_layers,\n",
        "        num_decoder_layers,\n",
        "        forward_expantion,\n",
        "        dropout,\n",
        "        max_len,\n",
        "        device,\n",
        "        norm_first=True\n",
        "    ):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_size)\n",
        "        self.src_positional_embedding = nn.Embedding(max_len, embedding_size)\n",
        "\n",
        "        self.tgt_word_embedding = nn.Embedding(tgt_vocab_size, embedding_size)\n",
        "        self.tgt_positional_embedding = nn.Embedding(max_len, embedding_size)\n",
        "\n",
        "        self.device = device\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.transformer = nn.Transformer(\n",
        "            embedding_size,\n",
        "            num_heads,\n",
        "            num_encoder_layers,\n",
        "            num_decoder_layers,\n",
        "            forward_expantion,\n",
        "            dropout,\n",
        "            norm_first=True\n",
        "        )\n",
        "\n",
        "        self.fc_out = nn.Linear(embedding_size, tgt_vocab_size)\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        # src shape: (src_len, N)\n",
        "        # src_mask shape: (N, src_len)\n",
        "        # maching required shape of src_key_padding_mask in nn.Transformer\n",
        "        src_mask = src.transpose(0, 1) == self.src_pad_idx\n",
        "        # src_mask shape: (N, src_len)\n",
        "\n",
        "        return src_mask.to(self.device)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_seq_length, N = src.shape\n",
        "        tgt_seq_length, N = tgt.shape\n",
        "\n",
        "        src_positions = (\n",
        "            torch.arange(0, src_seq_length)\n",
        "            .unsqueeze(1)\n",
        "            .expand(src_seq_length, N)\n",
        "            .to(self.device)\n",
        "        )\n",
        "\n",
        "        tgt_positions = (\n",
        "            torch.arange(0, tgt_seq_length)\n",
        "            .unsqueeze(1)\n",
        "            .expand(tgt_seq_length, N)\n",
        "            .to(self.device)\n",
        "        )\n",
        "\n",
        "        src_embedding = self.dropout(\n",
        "            (self.src_word_embedding(src) + self.src_positional_embedding(src_positions))\n",
        "        )\n",
        "\n",
        "        tgt_embedding = self.dropout(\n",
        "            (self.tgt_word_embedding(tgt) + self.tgt_positional_embedding(tgt_positions))\n",
        "        )\n",
        "\n",
        "        src_padding_mask = self.make_src_mask(src)\n",
        "        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt_seq_length).to(self.device)\n",
        "\n",
        "        out = self.transformer(\n",
        "            src_embedding,\n",
        "            tgt_embedding,\n",
        "            src_key_padding_mask=src_padding_mask,\n",
        "            tgt_mask=tgt_mask\n",
        "        )\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88676d21"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-07-21T20:21:09.208220Z",
          "start_time": "2022-07-21T20:21:09.198747Z"
        },
        "id": "3eb3f7d0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchtext.data.functional import load_sp_model, sentencepiece_tokenizer\n",
        "from torchtext.data.metrics import bleu_score\n",
        "import sys\n",
        "from random import random\n",
        "\n",
        "\n",
        "def translate(text, model, sp_model, source_vocab, target_vocab, device, max_length=100):\n",
        "\n",
        "    # Tokenize the text and lower-case it\n",
        "    source_sp_model = load_sp_model(sp_model)\n",
        "    sp_tokens_generator = sentencepiece_tokenizer(source_sp_model)\n",
        "    tokenized_text = sp_tokens_generator(text)\n",
        "\n",
        "    # [To-Do] Adjust to translate multiple sentences\n",
        "    tokenized_text = next(tokenized_text)\n",
        "    print(\"• Source text:\", tokenized_text, sep=\"\\n\")\n",
        "\n",
        "    tokenized_text = [src_code] + tokenized_text + [eos_code]\n",
        "    # print(tokenized_text)\n",
        "\n",
        "    # Convert text to indices\n",
        "    text_to_indices = source_vocab(tokenized_text)\n",
        "\n",
        "    # Convert to Tensor\n",
        "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    outputs = target_vocab([src_code])\n",
        "\n",
        "    for i in range(max_length):\n",
        "        trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(sentence_tensor, trg_tensor)\n",
        "\n",
        "        best_guess = output.argmax(2)[-1, :].item()\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "        if best_guess == target_vocab[\"</s>\"]:\n",
        "            break\n",
        "\n",
        "    target_vocab_itos = target_vocab.get_itos()\n",
        "    translated_sentence = [target_vocab_itos[idx] for idx in outputs]\n",
        "    # remove the start token\n",
        "    translated_sentence = translated_sentence[1:]\n",
        "    # remove the end token\n",
        "    if translated_sentence[-1] == \"</s>\":\n",
        "        translated_sentence = translated_sentence[:-1]\n",
        "    translated_sentence = \" \".join(translated_sentence).replace(\" \", \"\").replace(\"▁\", \" \")\n",
        "\n",
        "    return translated_sentence\n",
        "\n",
        "\n",
        "def bleu(data_iter, model, tokenizer, source_vocab, target_vocab, device):\n",
        "    targets = []\n",
        "    outputs = []\n",
        "\n",
        "    for source, target in data_iter:\n",
        "\n",
        "        prediction = translate(source, model, tokenizer, source_vocab, target_vocab, device)\n",
        "        prediction = prediction[:-1]  # remove <eos> token\n",
        "\n",
        "        targets.append([target])\n",
        "        outputs.append(prediction)\n",
        "\n",
        "    return bleu_score(outputs, targets)\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, mean_valid_loss):\n",
        "  checkpoint = {\"state_dict\":model.state_dict(),\n",
        "                \"opt\":optimizer.state_dict(),\n",
        "                \"epoch\":epoch,\n",
        "                \"valid_loss\":mean_valid_loss,\n",
        "                \"encoder_type\":\"transformer\",\n",
        "                }\n",
        "  checkpoint_path = f\"model_checkpoint.{epoch}.tar\"\n",
        "  torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "  return checkpoint_path\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_path, model, optimizer):\n",
        "  print(\"=> Loading checkpoint\")\n",
        "  checkpoint = torch.load(checkpoint_path)\n",
        "  model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "  optimizer.load_state_dict(checkpoint[\"opt\"])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def load_checkpoint_for_inference(checkpoint_path, model):\n",
        "  checkpoint = torch.load(checkpoint_path)\n",
        "  model.load_state_dict(checkpoint['state_dict'])\n",
        "  device_name = \"GPU\" if next(model.parameters()).is_cuda is True else \"CPU\"\n",
        "  print(\"Model checkpoint loaded to %s\" %device_name)\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "# Reference: https://stackoverflow.com/a/73704579/7614380\n",
        "class EarlyStopper:\n",
        "  def __init__(self, patience=1, min_delta=0):\n",
        "    self.patience = patience\n",
        "    self.min_delta = min_delta\n",
        "    self.counter = 0\n",
        "    self.min_validation_loss = float(\"inf\")\n",
        "\n",
        "  def early_stop(self, validation_loss):\n",
        "    if validation_loss < self.min_validation_loss:\n",
        "      self.min_validation_loss = validation_loss\n",
        "      self.counter = 0\n",
        "    elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
        "      self.counter += 1\n",
        "      if self.counter >= self.patience:\n",
        "        return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-07-21T20:21:09.213154Z",
          "start_time": "2022-07-21T20:21:09.209480Z"
        },
        "id": "314f4179"
      },
      "outputs": [],
      "source": [
        "# Validate the model\n",
        "\n",
        "def validate(valid_dataloader):\n",
        "\n",
        "  valid_dataloader = DataLoader(dev_dataset, collate_fn=collate_batch, batch_sampler=batch_sampler(dev_dataset))\n",
        "\n",
        "  model.eval() # prep model for evaluation\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    valid_losses = []\n",
        "\n",
        "    for source_batch, target_batch in valid_dataloader:\n",
        "      source = source_batch.to(device)\n",
        "      target = target_batch.to(device)\n",
        "\n",
        "      # Forward pass: compute predicted outputs by passing inputs to the model\n",
        "      output = model(source, target[:-1, :])\n",
        "      # Exclude the start token\n",
        "      output = output.reshape(-1, output.shape[2])\n",
        "      target = target[1:].reshape(-1)\n",
        "\n",
        "      # Calculate the loss\n",
        "      loss = criterion(output, target)\n",
        "      # Save the validation loss\n",
        "      valid_losses.append(loss.item())\n",
        "\n",
        "    mean_valid_loss = sum(valid_losses) / len(valid_losses)\n",
        "\n",
        "  return mean_valid_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45b4e96c"
      },
      "source": [
        "# Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-07-21T20:21:10.955384Z",
          "start_time": "2022-07-21T20:21:09.214094Z"
        },
        "id": "67eb05ed"
      },
      "outputs": [],
      "source": [
        "load_model = False\n",
        "\n",
        "# Training Hyperparameters\n",
        "num_epochs = 1000\n",
        "learning_rate = 3e-4\n",
        "batch_size = 128  # examples - make sure it is the same as in data preperation\n",
        "early_stopping_epochs = 4  # stop training if the validation loss is not improved after n epochs\n",
        "\n",
        "# Model Hyperparameters\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() == True else \"cpu\")\n",
        "src_vocab_size = len(source_vocab)\n",
        "tgt_vocab_size = len(target_vocab)\n",
        "embedding_size = 512\n",
        "num_heads = 8\n",
        "num_encoder_layers = 3  # 6\n",
        "num_decoder_layers = 3  # 6\n",
        "dropout = 0.1\n",
        "max_len = 200\n",
        "forward_expansion = 2048\n",
        "norm_first = True\n",
        "src_pad_idx = source_vocab[\"<pad>\"]\n",
        "\n",
        "\n",
        "# Tensorboard\n",
        "writer = SummaryWriter(f\"runs/loss_plot\")\n",
        "step = 0\n",
        "\n",
        "model = Transformer(\n",
        "  embedding_size,\n",
        "  src_vocab_size,\n",
        "  tgt_vocab_size,\n",
        "  src_pad_idx,\n",
        "  num_heads,\n",
        "  num_encoder_layers,\n",
        "  num_decoder_layers,\n",
        "  forward_expansion,\n",
        "  dropout,\n",
        "  max_len,\n",
        "  device,\n",
        "  norm_first=True,\n",
        ").to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "  optimizer, factor=0.1, patience=10, verbose=True\n",
        ")\n",
        "\n",
        "\n",
        "tgt_pad_idx = target_vocab[\"<pad>\"]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tgt_pad_idx)\n",
        "\n",
        "if load_model:\n",
        "  load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e937c22e"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-07-21T20:21:38.082900Z",
          "start_time": "2022-07-21T20:21:10.956580Z"
        },
        "id": "50a986b5",
        "outputId": "45bd7ec1-cfe5-4532-aa80-dce9fc81f91e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0 / 1000]\n",
            "Saving checkpoint...\n",
            "Saving checkpoint at: model_checkpoint.0.tar\n",
            "• Source text:\n",
            "['▁where', '▁is', '▁your', '▁school', '?']\n",
            "Translated text:\n",
            " هل هل هل هل هل هل هل هل هل هل هل؟\n",
            "Training Loss: 5.83\tValidation Loss: 5.46\n",
            "------------------------------\n",
            "Epoch [1 / 1000]\n",
            "Saving checkpoint...\n",
            "Saving checkpoint at: model_checkpoint.1.tar\n",
            "• Source text:\n",
            "['▁where', '▁is', '▁your', '▁school', '?']\n",
            "Translated text:\n",
            " هل هل؟\n",
            "Training Loss: 5.25\tValidation Loss: 5.05\n",
            "------------------------------\n",
            "Epoch [2 / 1000]\n",
            "Saving checkpoint...\n",
            "Saving checkpoint at: model_checkpoint.2.tar\n",
            "• Source text:\n",
            "['▁where', '▁is', '▁your', '▁school', '?']\n",
            "Translated text:\n",
            " هل تُِِكَكَكَكَكََكَكَِكََكَََِكَكََََََِِكََََِكََََََََََََََََََََََََََََََََََََََََََََََََََََِ\n",
            "Training Loss: 4.79\tValidation Loss: 4.84\n",
            "------------------------------\n",
            "Epoch [3 / 1000]\n",
            "Saving checkpoint...\n",
            "Saving checkpoint at: model_checkpoint.3.tar\n",
            "• Source text:\n",
            "['▁where', '▁is', '▁your', '▁school', '?']\n",
            "Translated text:\n",
            "كت إلى المنزل.\n",
            "Training Loss: 4.37\tValidation Loss: 4.54\n",
            "------------------------------\n",
            "Epoch [4 / 1000]\n",
            "Saving checkpoint...\n",
            "Saving checkpoint at: model_checkpoint.4.tar\n",
            "• Source text:\n",
            "['▁where', '▁is', '▁your', '▁school', '?']\n",
            "Translated text:\n",
            "كتك فيك؟\n",
            "Training Loss: 3.99\tValidation Loss: 4.3\n",
            "------------------------------\n",
            "Epoch [5 / 1000]\n",
            "Saving checkpoint...\n",
            "Saving checkpoint at: model_checkpoint.5.tar\n",
            "• Source text:\n",
            "['▁where', '▁is', '▁your', '▁school', '?']\n",
            "Translated text:\n",
            " أين هو؟\n",
            "Training Loss: 3.65\tValidation Loss: 4.06\n",
            "------------------------------\n",
            "Epoch [6 / 1000]\n",
            "Saving checkpoint...\n",
            "Saving checkpoint at: model_checkpoint.6.tar\n",
            "• Source text:\n",
            "['▁where', '▁is', '▁your', '▁school', '?']\n",
            "Translated text:\n",
            " أين مدرستك؟\n",
            "Training Loss: 3.32\tValidation Loss: 3.9\n",
            "------------------------------\n",
            "Epoch [7 / 1000]\n",
            "Saving checkpoint...\n",
            "Saving checkpoint at: model_checkpoint.7.tar\n",
            "• Source text:\n",
            "['▁where', '▁is', '▁your', '▁school', '?']\n",
            "Translated text:\n",
            "؟ي المدرسةك؟\n",
            "Training Loss: 3.0\tValidation Loss: 3.75\n",
            "------------------------------\n",
            "Epoch [8 / 1000]\n",
            "Saving checkpoint...\n",
            "Saving checkpoint at: model_checkpoint.8.tar\n",
            "• Source text:\n",
            "['▁where', '▁is', '▁your', '▁school', '?']\n",
            "Translated text:\n",
            " أين مدرسك؟\n",
            "Training Loss: 2.71\tValidation Loss: 3.62\n",
            "------------------------------\n",
            "Epoch [9 / 1000]\n",
            "Saving checkpoint...\n",
            "Saving checkpoint at: model_checkpoint.9.tar\n",
            "• Source text:\n",
            "['▁where', '▁is', '▁your', '▁school', '?']\n",
            "Translated text:\n",
            "؟ أين مدرستك؟\n",
            "Training Loss: 2.44\tValidation Loss: 3.58\n",
            "------------------------------\n",
            "Epoch [10 / 1000]\n",
            "Saving checkpoint...\n",
            "Saving checkpoint at: model_checkpoint.10.tar\n",
            "• Source text:\n",
            "['▁where', '▁is', '▁your', '▁school', '?']\n",
            "Translated text:\n",
            " أين هو مدرستك؟\n",
            "Training Loss: 2.2\tValidation Loss: 3.57\n",
            "------------------------------\n",
            "Epoch [11 / 1000]\n",
            "Saving checkpoint...\n",
            "Saving checkpoint at: model_checkpoint.11.tar\n",
            "• Source text:\n",
            "['▁where', '▁is', '▁your', '▁school', '?']\n",
            "Translated text:\n",
            " أين مدرستك؟\n",
            "Training Loss: 1.96\tValidation Loss: 3.49\n",
            "------------------------------\n",
            "Epoch [12 / 1000]\n",
            "Validation loss did not improve from the best model!\n",
            "• Source text:\n",
            "['▁where', '▁is', '▁your', '▁school', '?']\n",
            "Translated text:\n",
            " أين مدرستك؟\n",
            "Training Loss: 1.78\tValidation Loss: 3.61\n",
            "------------------------------\n",
            "Epoch [13 / 1000]\n",
            "Validation loss did not improve from the best model!\n",
            "• Source text:\n",
            "['▁where', '▁is', '▁your', '▁school', '?']\n",
            "Translated text:\n",
            " هل أين مدرستك؟\n",
            "Training Loss: 1.6\tValidation Loss: 3.67\n",
            "------------------------------\n",
            "Epoch [14 / 1000]\n",
            "Validation loss did not improve from the best model!\n",
            "• Source text:\n",
            "['▁where', '▁is', '▁your', '▁school', '?']\n",
            "Translated text:\n",
            " أين مدرستك؟\n",
            "Training Loss: 1.42\tValidation Loss: 3.52\n",
            "------------------------------\n",
            "Early stopping. The best model has been saved.\n"
          ]
        }
      ],
      "source": [
        "sentence = [\"where is your school?\"]\n",
        "\n",
        "best_model_loss = float(\"inf\")\n",
        "\n",
        "early_stopper = EarlyStopper(patience=early_stopping_epochs, min_delta=0)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  print(f\"Epoch [{epoch} / {num_epochs}]\")\n",
        "\n",
        "  train_dataloader = DataLoader(train_dataset, collate_fn=collate_batch, batch_sampler=batch_sampler(train_dataset))\n",
        "\n",
        "  # important if model.eval() was called earlier as in translate()\n",
        "  model.train()\n",
        "\n",
        "  train_losses = []\n",
        "\n",
        "  for b, (source_batch, target_batch) in enumerate(train_dataloader):\n",
        "    source = source_batch.to(device)\n",
        "    target = target_batch.to(device)\n",
        "\n",
        "    # Forward propagation\n",
        "    output = model(source, target[:-1, :])\n",
        "    # output shape: (target_len, batch_size, output_dim)\n",
        "\n",
        "    # Exclude the start token\n",
        "    # Reshape to match the accepted input form of CrossEntropyLoss\n",
        "    # Keep the output dimention (whose size is tgt_vocab_size, for the probability of each token)...\n",
        "    # and flatten the two first dimentions\n",
        "    output = output.reshape(-1, output.shape[2])\n",
        "    target = target[1:].reshape(-1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(output, target)\n",
        "    train_losses.append(loss.item())\n",
        "\n",
        "    # Back propagation\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip to avoid exploding gradients, makes sure grads are within a healthy range\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "    # Gradient descent step\n",
        "    optimizer.step()\n",
        "\n",
        "    writer.add_scalar(\"Training Loss\", loss, global_step=step)\n",
        "    step += 1\n",
        "\n",
        "  mean_train_loss = sum(train_losses) / len(train_losses)\n",
        "  scheduler.step(mean_train_loss)\n",
        "\n",
        "\n",
        "  ### Validate the model\n",
        "  mean_valid_loss = validate(valid_dataloader)\n",
        "\n",
        "  # Save the best checkpoint\n",
        "  if mean_valid_loss < best_model_loss:\n",
        "    best_model_loss = mean_valid_loss\n",
        "    checkpoint_path = save_checkpoint(model, optimizer, epoch, mean_valid_loss)\n",
        "    print(\"Model checkpoint saved at:\", checkpoint_path)\n",
        "  else:\n",
        "    print(\"Validation loss did not improve from the best model!\")\n",
        "\n",
        "  # Translate the example sentence\n",
        "  translated_sentence = translate(sentence, model, \"source.spm.model\", source_vocab, target_vocab, device, max_length=100)\n",
        "  print(\"Translated text:\", translated_sentence, sep=\"\\n\")\n",
        "\n",
        "  print(\"Training Loss: %s\" %round(mean_train_loss, 2), \"Validation Loss: %s\" %round(mean_valid_loss, 2), sep=\"\\t\")\n",
        "  print(\"---\"*10)\n",
        "\n",
        "  # Early Stopping, if the validation loss is not improving\n",
        "  if early_stopper.early_stop(mean_valid_loss):\n",
        "    print(\"Early stopping. The best model has been saved at:\", checkpoint_path)\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ddcf018"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c15bc686"
      },
      "source": [
        "# Translation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = load_checkpoint_for_inference(checkpoint_path, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJW9XxmEvkIn",
        "outputId": "ee162e24-c30a-4c53-d870-dce08bf221f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model checkpoint loaded to GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = [\"where is your school?\"]\n",
        "translate(sentence, loaded_model, \"source.spm.model\", source_vocab, target_vocab, device, max_length=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "qqMNUWCEyBP8",
        "outputId": "7c19f97e-0a33-45be-aa07-49efc3fca806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "• Source text:\n",
            "['▁where', '▁is', '▁your', '▁school', '?']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' أين مدرستك؟'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bf3316e"
      },
      "source": [
        "### To-do\n",
        "\n",
        "* [DONE] Prenorm in nn.Transformer (norm_first=True)\n",
        "* Arrange sentences by length in one batch (<a href=\"https://github.com/pytorch/text/blob/master/examples/legacy_tutorial/migration_tutorial.ipynb\">example</a>)\n",
        "* Beam Search at inference time\n",
        "* try: train_dataset,test_dataset=torch.utils.data.random_split(ants_dataset,(train_length,test_length))\n",
        "* Add (more) validation data"
      ]
    }
  ]
}